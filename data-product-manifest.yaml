version: 0.0.1
jobId: "996"
jobName: Delete_DataProd
jobType: Source Aligned Data Product
developerId: fb302eb3-31f0-4180-8dcd-ff6b2305ad54
domain: manufacturing
alias: writetoicebergdelete
discoveryPort:
  name: Delete_DataProd
inputPorts:
  - alias: GCP_dabur_1
    isDynamic: true
    path: gs://dabur_poc_inutdata/weather_data/20240118095755.csv
    optional:
      persistDataFrame: false
      advanceOptions:
        delimiter: ","
      enableDataReconciliation: false
      enforceSchema: false
      connection: GCP_Ayush
      dataSetUrn: urn:dv:dataset:f3ad834c-377d-4c41-9840-d1da484b01d1
    type: inputDelimited
productState:
  isDynamic: true
  alias: writetoicebergdelete
  retentionVersions: ""
  logicalSchema:
    properties:
      REGION:
        type: STRING
        description: ""
      ZSMCODE:
        type: STRING
        description: ""
      rowID:
        type: STRING
        description: ""
  stateStoreType: loadDataIceberg
  isProfilingEnabled: false
  tableName: s3Source.newdataset
  warehousePath: s3://bp-spark-sql-library-test-acc/
  catalogName: glue
  optional:
    persistDataFrame: false
    enableDataReconciliation: false
    enforceSchema: true
    enforceSchemaMethod: Warning
    catalogType: glue
  refreshInterval: None
transformation:
  - isDynamic: true
    alias: Spark_SQL_1
    description: loadsparkdeletedata
    sequence: 2
    inputDataFrameList:
      - inputDataFrame: GCP_dabur_1
        tempViewName: newdataset
    query: select * from newdataset
    optional:
      persistDataFrame: false
    type: operationThroughSqlQuery
controlPort:
  dataQualityRules: {}
